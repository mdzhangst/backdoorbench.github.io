<!DOCTYPE html>
<html lang="en">

<head>
    <script src="https://kit.fontawesome.com/15106f0a68.js" crossorigin="anonymous"></script>
    <meta charset="utf-8">
    <title>BackdoorBench</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description"
        content="BackdoorBench is a evaluation benchmark to compare the results from different attack and defense methods, and provide a easy implememntation for ones who want to replicate classic backdoor methods.">
    <meta name="keywords" content="Backdoor Benchmark, Backdoor Attack, Backdoor Defense, AI Security, Adversarial">
    <meta name="author" content="BackdoorBench">
    <link rel="icon" href="image/favicon.ico">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/fonts.css">
    <link rel="stylesheet" href="css/custom.css">
</head>

<body>
    <nav class="navbar navbar-expand-lg navbar-dark navbar-custom fixed-top">
        <div class="container">
            <a class="navbar-brand" href="index.html"><b>BackdoorBench</b></a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#content"
                aria-controls="navbarsExample07" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="content">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item active">
                        <a class="nav-link" href="index.html"><b>Home</b></a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="https://github.com/SCLBD/bdzoo2/tree/merged" target="_blank">Code</a>
                    </li>

                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button"
                            data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                            Leaderboard
                        </a>
                        <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                            <a class="dropdown-item" href="leaderboard-cifar10.html">CIFAR-10</a>
                            <a class="dropdown-item" href="leaderboard-gtsrb.html">GTSRB</a>
                            <a class="dropdown-item" href="leaderboard-tinyimagenet.html">Tiny ImageNet</a>
                        </div>
                    </li>

                    <li class="nav-item">
                        <a class="nav-link" href="challenge.html">Challenge</a>
                    </li>
                    <!-- <li class="nav-item">
                            <a class="nav-link" href="https://arxiv.org" target="_blank">Paper</a>
                        </li> -->
                    <li class="nav-item">
                        <a class="nav-link" href="people.html">People</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <br />
    <br />

    <div class="container" align="justify">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <center>
                    <h3>Will be coming soon...</h3>
                </center>
            </div>
        </div>
    </div>

    <br />

    <div class="container" align="justify">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h4>Overview</h4>
            </div>
        </div>

        <hr>

        <!-- <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <p>We are pleased to announce VALUE Challenge 2021! The challenge will be hosted at the <a
                        href="https://sites.google.com/view/iccv21clvl/home" target="_blank">Forth Workshop on Closing
                        the Loop Between Vision and Language</a>, ICCV 2021.</p>
                <p>Please stay tuned for more information!</p>
            </div>
        </div> -->
    </div>

    <br />

    <div class="container" align="justify">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h4>Important Dates</h4>
            </div>
        </div>

        <hr>

        <!-- <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <ul>
                    <li>Challenge Launch:
                        <s>
                        <b>June 7th, 2021.</b>
                        </s>
                    </li>
                    <li>Results Submission Deadline: <b>23:59:59 (AoE), September 13th, 2021.</b></li>
                    <li>Decision to participants: <b>September 27th, 2021.</b></li>
                    <li>Challenge Paper Submission Deadline: <b>June 8th, 2020.</b></li>
                    <li>The winners will be announced at the CLVL workshop, ICCV 2021
                        on October 17th, 2021
                        .</li>
                </ul>
            </div>
        </div> -->
    </div>

    <br />
    <div class="container" align="justify">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h4>Important Updates</h4>
            </div>
        </div>

        <hr>

        <!-- <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <ul>
                    <li><span style="font-size: 14px;" class="date badge badge-secondary">[09/07/2021]</span>
                        <font color="red">We noticed an error in calculating the meta average score in our CodaLab
                            evaluation script</font>, where 11 tasks across 3 macro-tasks and the 3 average scores of
                        the 3 macro-tasks are used, thus leading to the wrong results. By definition, meta average score
                        should be the average of scores from the 11 tasks. We have updated the evaluation script to
                        correctly calculate the meta average score. Note that your new submissions from today will be
                        evaluated using this script, while old submissions scores will not be updated, you need to
                        calculate it yourself.
                    </li>
                    <li><span style="font-size: 14px;" class="date badge badge-secondary">[08/23/2021]</span> CodaLab
                        evaluation portal updates: (1) support `leaderboard` feature, you will now see a `Submit to
                        Leaderboard` button to allow display your results on CodaLab's leaderboard; (2) the server will
                        only evaluate `test` splits, instead of both `val` and `test` to reduce evaluation time cost;
                        (3) action required: please register and submit predictions to <a
                            href="https://competitions.codalab.org/competitions/34470">this new portal</a>, the <a
                            href="https://competitions.codalab.org/competitions/32417">old portal</a> will be deleted
                        soon.</li>
                    <li><span style="font-size: 14px;" class="date badge badge-secondary">[08/05/2021]</span> We updated
                        the main metric to <font color="red">Mean-Rank</font> for all leaderboards (challenge phases).
                        Mean-Rank is the average of model rank on each task considered in the leaderboard or challenge
                        phase. Browse each <a href="leaderboard.html">leaderboard</a> to see how VALUE baselines are
                        ranked in action. <br>
                        Q: Why we changed the main metric? A: Check out full discussions <a
                            href="https://openreview.net/forum?id=9E3dTIMxL8S&noteId=jC30l0UTYJe">here</a>.</li>
                </ul>
            </div>
        </div> -->
    </div>

    <br />
    <div class="container" align="justify">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h4>Challenge Submission Requirements</h4>
            </div>
        </div>

        <hr>

        <!-- <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <p>To be eligible for consideration for awards, we kindly request you to fill out <a
                        href="https://forms.gle/iYWT4VZzfXzUvPus8">this Google form</a>, and then forward a copy of your
                    response to value-benchmark@googlegroups.com for us to confirm.
                </p>
            </div>
        </div> -->
    </div>

    <br />


    <div class="container" align="justify">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h4>Guidelines</h4>
            </div>
        </div>

        <hr>

        <!-- <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <p>The VALUE benchmark is a collection of video-and-language dataset on multi-channel videos
                    (video+subtitle) across diverse video domains and genres, which contains 11 datasets over 3 popular
                    video-and-language tasks: text-based video retrieval, video question answering and video captioning.
                    Please refer to our <a href="" target="_blank">paper</a> for more details. This VALUE Challenge aims
                    to benchmark progress towards general video-and-language understanding systems that can generalize
                    to different tasks and can process multi-channel videos with both visual frames and subtitles as
                    inputs.</p>
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h5>Dataset Download</h5>
                <p>Please refer to the details at the <b><a href="https://github.com/VALUE-Leaderboard/DataRelease"
                            target="_blank">Data Release</a></b> repo. You can download textual annotations, subtitles
                    and different visual features of each dataset following the instructions.</p>
            </div>
            <br />

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h5>Challenge Phases</h5>
                <ul>
                    <li><b>VALUE</b>: This phase evaluates <b>task-agnostic</b> algorithms on all datasets and tasks in
                        VALUE benchmark. A submission needs to consist of results on all 11 datasets to be considered as
                        a valid submission.</li>
                    <li><b>Retrieval</b>: This phase evaluates algorithms on 4 text-to-video retrieval tasks in VALUE
                        benchmark, including TVR, How2R, YC2R and VATEX-EN-R. A submission needs to consist of results
                        on all 4 datasets to be considered as a valid submission. </li>
                    <li><b>QA</b>: This phase evaluates algorithms on 4 video question answering tasks in VALUE
                        benchmark, including TVQA, How2QA, VIOLIN and VLEP. A submission needs to consist of results on
                        all 4 datasets to be considered as a valid submission. </li>
                    <li><b>Captioning</b>: This phase evaluates algorithms on 3 video captioning tasks in VALUE
                        benchmark, including TVC, YC2C and VATEX-EN-C. A submission needs to consist of results on all 3
                        datasets to be considered as a valid submission. </li>
                </ul>
                <p>Please check out <b><a href="submission.html" target="_blank">VALUE Submission</a></b> page for more
                    instructions on submission.</p>
            </div>
            <br />


            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h5>Challenge Prizes</h5>
                <p>The top ranked participants from each challenge phase will be awarded with <a
                        href="https://azure.microsoft.com/en-us/" target="_blank">Microsoft Azure</a> credit. The prizes
                    are $9,000 for VALUE phase and $4,500 for the other phases.</p>
            </div>
            <br />
        </div> -->
    </div>
    <br />


    <div class="container">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h4>Organizers</h4>
                <hr>
            </div>
        </div>
        <!-- <div class="row">
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <img src="image/people/baoyuan wu.jpg" class="rounded img-fluid mx-auto d-block" style="width:11rem;"/>
                    <p class="text-center" style="height:5rem;"><b><a href="https://eric-xw.github.io/" target="_block">Xin (Eric) Wang</a></b><br/>UC Santa Cruz</p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <img src="image/jiawei_wu.jpg" class="rounded img-fluid mx-auto d-block" style="width:11rem;"/>
                    <p class="text-center" style="height:5rem;"><b><a href="http://jiawei-wu.com/" target="_block">Jiawei Wu</a></b><br/>Shannon AI</p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <img src="image/junkun_chen.jpeg" class="rounded img-fluid mx-auto d-block" style="width:11rem;"/>
                    <p class="text-center" style="height:5rem;"><b><a href="https://web.engr.oregonstate.edu/~chenjun2/" target="_block">Junkun Chen</a></b><br/>Oregon State University</p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <img src="image/lei_li.jpg" class="rounded img-fluid mx-auto d-block" style="width:11rem;"/>
                    <p class="text-center" style="height:5rem;"><b><a href="https://lileicc.github.io/" target="_block">Lei Li</a></b><br/>ByteDance AI Lab</p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <img src="image/yaunfang_wang.jpeg" class="rounded img-fluid mx-auto d-block" style="width:11rem;"/>
                    <p class="text-center" style="height:5rem;"><b><a href="https://sites.cs.ucsb.edu/~yfwang/" target="_block">Yuan-Fang Wang</a></b><br/>UCSB</p>
                </div>

                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <img src="image/william_wang.jpg" class="rounded img-fluid mx-auto d-block" style="width:11rem;"/>
                    <p class="text-center" style="height:5rem;"><b><a href="http://www.cs.ucsb.edu/~william/" target="_block">William Yang Wang</a></b><br/>UC Santa Barbara</p>
                </div>
            </div> -->
    </div>

    <br />

    <div class="container" align="justify">
        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <h4>Contact</h4>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                <p>
                    Have any questions or suggestions? Feel free to contact us at <a
                        href="mailto:liuxudongbupt@163.com">wubaoyuan@cuhk.edu.cn</a> !
                </p>
            </div>
        </div>

    </div>

    <br />

</body>

<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
    crossorigin="anonymous"></script>
<script>window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')</script>
<script src="js/popper.min.js"></script>
<script src="js/bootstrap.min.js"></script>

</html>